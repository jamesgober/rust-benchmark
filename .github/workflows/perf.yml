name: Perf

on:
  schedule:
    # Sunday 02:00 UTC
    - cron: "0 2 * * 0"
    # Wednesday 02:00 UTC
    - cron: "0 2 * * 3"
  workflow_dispatch: {}

jobs:
  perf-tests:
    name: Perf Tests and Benches
    runs-on: ubuntu-latest
    # Only run on main branch for scheduled/manual invocations
    if: github.ref_name == 'main'
    concurrency:
      group: perf-${{ github.ref }}
      cancel-in-progress: false
    timeout-minutes: 45
    env:
      PERF_TESTS: "1"
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      # Optional diagnostics for visibility
      - name: Environment
        run: |
          rustc -Vv
          cargo -V
          lscpu || true
          cat /etc/os-release || true

      - name: Build (all-features) for benches
        run: cargo build --all-features

      - name: Run perf-gated tests (ignored by default)
        run: cargo test -F perf-tests -- --ignored

      - name: Run perf-gated benches
        run: cargo bench -F perf-tests

      - name: Install jq (for baseline comparison)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq python3

      - name: Run watch_timer_hot (metrics) with fixed timings
        env:
          PERF_TESTS: "1"
        run: |
          cargo bench -F "perf-tests metrics" --bench watch_timer_hot -- \
            --measurement-time 5 \
            --warm-up-time 2 \
            --save-baseline current

      - name: Show Criterion output (watch_timer_hot)
        run: |
          echo "Listing target/criterion (if present)"
          ls -la target || true
          ls -la target/criterion || true
          find target/criterion -maxdepth 2 -type d -print || true

      - name: Compare Criterion results to baselines (watch_timer_hot)
        run: |
          bash scripts/compare_criterion_baseline.sh watch_timer_hot perf_baselines/watch_timer_hot.json

      - name: Run timers (perf) with fixed timings
        env:
          PERF_TESTS: "1"
        run: |
          cargo bench -F perf-tests --bench timers -- \
            --measurement-time 5 \
            --warm-up-time 2 \
            --save-baseline current

      - name: Compare Criterion results to baselines (timers)
        run: |
          bash scripts/compare_criterion_baseline.sh timers perf_baselines/timers.json

      - name: Run histogram_hot (perf) with fixed timings
        env:
          PERF_TESTS: "1"
        run: |
          cargo bench -F perf-tests --bench histogram_hot -- \
            --measurement-time 5 \
            --warm-up-time 2 \
            --save-baseline current

      - name: Compare Criterion results to baselines (histogram_hot)
        run: |
          bash scripts/compare_criterion_baseline.sh histogram_hot perf_baselines/histogram_hot.json
